{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* the pdf with your answers\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Python 3.6 or above is required\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = Path('/home/tam/data/')\n",
    "# Download word vectors, might take a few minutes and about ~3GB of storage space\n",
    "en_embeddings_path = PATH_TO_DATA / 'cc.en.300.vec.gz'\n",
    "if not en_embeddings_path.exists():\n",
    "    urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz', en_embeddings_path)\n",
    "fr_embeddings_path = PATH_TO_DATA / 'cc.fr.300.vec.gz'\n",
    "if not fr_embeddings_path.exists():\n",
    "    urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz', fr_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec():\n",
    "\n",
    "    def __init__(self, filepath, vocab_size=50000):\n",
    "        self.words, self.embeddings = self.load_wordvec(filepath, vocab_size)\n",
    "        # Mappings for O(1) retrieval:\n",
    "        self.word2id = {word: idx for idx, word in enumerate(self.words)}\n",
    "        self.id2word = {idx: word for idx, word in enumerate(self.words)}\n",
    "    \n",
    "    def load_wordvec(self, filepath, vocab_size):\n",
    "        assert str(filepath).endswith('.gz')\n",
    "        words = []\n",
    "        embeddings = []\n",
    "        with gzip.open(filepath, 'rt') as f:  # Read compressed file directly\n",
    "            next(f)  # Skip header\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                words.append(word)\n",
    "                embeddings.append(np.fromstring(vec, sep=' '))\n",
    "                if i == (vocab_size - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(words)))\n",
    "        return words, np.vstack(embeddings)\n",
    "    \n",
    "    def encode(self, word):\n",
    "        # Returns the 1D embedding of a given word\n",
    "        idx = self.word2id.get(word, -1)\n",
    "        if idx==-1:\n",
    "            return np.zeros(300)\n",
    "        else:\n",
    "            vec = self.embeddings[idx]\n",
    "            return vec\n",
    "    \n",
    "    def score(self, word1, word2):\n",
    "        # Return the cosine similarity: use np.dot & np.linalg.norm\n",
    "        vec1 = self.encode(word1)\n",
    "        vec2 = self.encode(word2)\n",
    "        cos_sim = np.dot(vec1, vec2)\n",
    "        cos_sim = cos_sim / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        return cos_sim\n",
    "    \n",
    "    def most_similar(self, word, k=5):\n",
    "        # Returns the k most similar words: self.score & np.argsort \n",
    "        scores = [self.score(word, self.id2word[i]) for i in range(len(self.words))]\n",
    "        indexes = list(np.flip(np.argsort(scores)))\n",
    "        return [self.id2word[i] for i in indexes[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "cat tree 0.26449754661654756\n",
      "cat dog 0.7078641298542564\n",
      "cat pet 0.6753313359976382\n",
      "Paris France 0.6892958925806543\n",
      "Paris Germany 0.4051242286737549\n",
      "Paris baguette 0.29399958277802224\n",
      "Paris donut -0.006588507552348003\n",
      "['cat', 'cats', 'kitty', 'kitten', 'feline']\n",
      "['dog', 'dogs', 'puppy', 'pup', 'canine']\n",
      "['dogs', 'dog', 'cats', 'puppies', 'Dogs']\n",
      "['Paris', 'France', 'Parisian', 'Marseille', 'Brussels']\n",
      "['Germany', 'Austria', 'Europe', 'Berlin', 'Hamburg']\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for word1, word2 in zip(('cat', 'cat', 'cat', 'Paris', 'Paris', 'Paris', 'Paris'), ('tree', 'dog', 'pet', 'France', 'Germany', 'baguette', 'donut')):\n",
    "    print(word1, word2, word2vec.score(word1, word2))\n",
    "for word in ['cat', 'dog', 'dogs', 'Paris', 'Germany']:\n",
    "    print(word2vec.most_similar(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class BagOfWords():\n",
    "    \n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        # -> idf = {word: idf_value, ...}\n",
    "        idf = {}\n",
    "        D = len(sentences)\n",
    "        for sentence in sentences:\n",
    "            for word in set(sentence.split(' ')):\n",
    "                idf[word] = idf.get(word, 0) + 1\n",
    "                \n",
    "        for word in idf.keys():\n",
    "            idf[word] = np.log(D / (idf[word]))\n",
    "\n",
    "        return idf\n",
    "    \n",
    "    def encode(self, sentence, idf=None):\n",
    "        # Takes a sentence as input, returns the sentence embedding\n",
    "        if idf is None:\n",
    "            words_vec = np.array([word2vec.encode(word) for word in sentence.split(' ')])\n",
    "            return words_vec.mean(axis=0)\n",
    "        else:\n",
    "            # idf-weighted mean of word vectors\n",
    "            words_vec = np.array([word2vec.encode(word) for word in sentence.split(' ')])\n",
    "            idf_weights = np.array([idf[word] for word in sentence.split(' ')])\n",
    "            words_vec = np.average(words_vec, weights=idf_weights, axis=0)\n",
    "            return words_vec\n",
    "\n",
    "    def score(self, sentence1, sentence2, idf=None):\n",
    "        # cosine similarity: use np.dot & np.linalg.norm \n",
    "        vec1 = self.encode(sentence1, idf)\n",
    "        vec2 = self.encode(sentence2, idf)\n",
    "        cos_sim = np.dot(vec1, vec2)\n",
    "        cos_sim = cos_sim / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        return cos_sim\n",
    "    \n",
    "    def most_similar(self, sentence, sentences, idf=None, k=5):\n",
    "        # Return most similar sentences\n",
    "        scores = [self.score(sentence, sentence_) for sentence_ in sentences]\n",
    "        indexes = np.flip(np.argsort(scores))\n",
    "        return [sentences[i] for i in indexes[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "\n",
      "\tAverage of word embeddings\n",
      "1 man singing and 1 man playing a saxophone in a concert . \n",
      "10 people venture out to go crosscountry skiing . \n",
      "0.7065220648251476\n",
      "1 smiling african american boy . \n",
      "1) 1 smiling african american boy . \n",
      "2) 2 woman dancing while pointing . \n",
      "3) 5 women and 1 man are smiling for the camera . \n",
      "4) a small boy following 4 geese . \n",
      "5) 2 female babies eating chips . \n",
      "\n",
      "\tidf weighted average of word embeddings\n",
      "1 man singing and 1 man playing a saxophone in a concert . \n",
      "10 people venture out to go crosscountry skiing . \n",
      "0.6400799939346385\n",
      "1 smiling african american boy . \n",
      "1) 1 smiling african american boy . \n",
      "2) 2 woman dancing while pointing . \n",
      "3) 5 women and 1 man are smiling for the camera . \n",
      "4) a small boy following 4 geese . \n",
      "5) 2 female babies eating chips . \n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "sentence2vec = BagOfWords(word2vec)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "filepath = PATH_TO_DATA / 'sentences.txt'\n",
    "with open(filepath, 'r') as f:\n",
    "    sentences = [line.strip('\\n') for line in f]\n",
    "\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print('\\n\\tAverage of word embeddings')\n",
    "sentence1 = sentences[7]\n",
    "sentence2 = sentences[13]\n",
    "print(sentence1)\n",
    "print(sentence2)\n",
    "print(sentence2vec.score(sentence1, sentence2, idf=None))\n",
    "sentence = sentences[10]\n",
    "similar_sentences = sentence2vec.most_similar(sentence, sentences)  # BagOfWords-mean\n",
    "print(sentence)\n",
    "for i, sentence_ in enumerate(similar_sentences):\n",
    "    print(str(i+1) + ')', sentence_)\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = sentence2vec.build_idf(sentences)\n",
    "\n",
    "print('\\n\\tidf weighted average of word embeddings')\n",
    "print(sentence1)\n",
    "print(sentence2)\n",
    "print(sentence2vec.score(sentence1, sentence2, idf))\n",
    "similar_sentences = sentence2vec.most_similar(sentence, sentences, idf)  # BagOfWords-idf\n",
    "print(sentence)\n",
    "for i, sentence_ in enumerate(similar_sentences):\n",
    "    print(str(i+1) + ')', sentence_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualWordAligner:\n",
    "    \n",
    "    def __init__(self, fr_word2vec, en_word2vec):\n",
    "        self.fr_word2vec = fr_word2vec\n",
    "        self.en_word2vec = en_word2vec\n",
    "        self.aligned_fr_embeddings = self.get_aligned_fr_embeddings()\n",
    "        \n",
    "    def get_aligned_fr_embeddings(self):\n",
    "        # 1 - Get words that appear in both vocabs (= identical character strings)\n",
    "        #     Use it to create the matrix X (emb_dim, vocab_size) and Y (emb_dim, vocab_size) (of embeddings for these words)\n",
    "        fr_en_words = list(set(self.fr_word2vec.words).intersection(self.en_word2vec.words))\n",
    "        \n",
    "        # embedding matrix for french words\n",
    "        X = np.vstack([fr_word2vec.encode(word) for word in fr_en_words]).T\n",
    "        # embedding matrix for english words\n",
    "        Y = np.vstack([en_word2vec.encode(word) for word in fr_en_words]).T\n",
    "        \n",
    "        assert X.shape[0] == 300 and Y.shape[0] == 300\n",
    "        # 2 - Solve the Procrustes using the numpy package and: np.linalg.svd() and get the optimal W\n",
    "        #     Now self.fr_word2vec.embeddings * W.transpose() is in the same space as en_word2vec.embeddings\n",
    "        M = np.dot(Y, X.T)\n",
    "        u, s, vh = np.linalg.svd(M)\n",
    "        W = np.dot(u, vh)\n",
    "        assert W.shape == (300, 300)\n",
    "        return np.matmul(fr_word2vec.embeddings, W.transpose())\n",
    "        \n",
    "    def get_closest_english_words(self, fr_word, k=3):\n",
    "        # 3 - Return the top k English nearest neighbors to the input French word\n",
    "        idx = self.fr_word2vec.word2id[fr_word]\n",
    "        \n",
    "        # map the french word to the english space\n",
    "        similarity = self.get_aligned_fr_embeddings()\n",
    "        fr_vec = similarity[idx]\n",
    "        \n",
    "        similarity = self.en_word2vec.embeddings\n",
    "        # compute the norms\n",
    "        norm = np.linalg.norm(similarity, axis=1)\n",
    "        fr_word_norm = np.linalg.norm(fr_vec)\n",
    "        norm = norm * fr_word_norm\n",
    "        # then the cosine similarity\n",
    "        similarity = np.dot(similarity, fr_vec)\n",
    "        similarity = similarity / norm\n",
    "        \n",
    "        indexes = np.flip(np.argsort(similarity))\n",
    "        return [self.en_word2vec.words[i] for i in indexes[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n",
      "----------\n",
      "fr: \"chat\"\n",
      "en: \"cat\"\n",
      "en: \"kitten\"\n",
      "en: \"kitty\"\n",
      "----------\n",
      "fr: \"chien\"\n",
      "en: \"dog\"\n",
      "en: \"cat\"\n",
      "en: \"pet\"\n",
      "----------\n",
      "fr: \"voiture\"\n",
      "en: \"car\"\n",
      "en: \"vehicle\"\n",
      "en: \"automobile\"\n",
      "----------\n",
      "fr: \"zut\"\n",
      "en: \"oops\"\n",
      "en: \"Ah\"\n",
      "en: \"ah\"\n"
     ]
    }
   ],
   "source": [
    "fr_word2vec = Word2Vec(fr_embeddings_path, vocab_size=50000)\n",
    "en_word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "multilingual_word_aligner = MultilingualWordAligner(fr_word2vec, en_word2vec)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "fr_words = ['chat', 'chien', 'voiture', 'zut']\n",
    "k = 3\n",
    "for fr_word in fr_words:\n",
    "    print('-' * 10)\n",
    "    print(f'fr: \"{fr_word}\"')\n",
    "    en_words = multilingual_word_aligner.get_closest_english_words(fr_word, k=3)\n",
    "    for en_word in en_words:\n",
    "        print(f'en: \"{en_word}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "train_filepath = PATH_TO_DATA / 'SST/stsa.fine.train'\n",
    "dev_filepath = PATH_TO_DATA / 'SST/stsa.fine.dev'\n",
    "test_filepath = PATH_TO_DATA / 'SST/stsa.fine.test.X'\n",
    "\n",
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "\n",
    "# train sentences\n",
    "train_raw = []\n",
    "Y_train = []\n",
    "with open(train_filepath, 'r') as f:\n",
    "    for line in f:\n",
    "        Y_train.append(int(line[0]))\n",
    "        train_raw.append(line[2:])\n",
    "\n",
    "# dev sentences\n",
    "dev_raw = []\n",
    "Y_dev = []\n",
    "with open(dev_filepath, 'r') as f:\n",
    "    for line in f:\n",
    "        Y_dev.append(int(line[0]))\n",
    "        dev_raw.append(line[2:])\n",
    "\n",
    "        # test sentences\n",
    "test_raw = []\n",
    "with open(test_filepath, 'r') as f:\n",
    "    for line in f:\n",
    "        test_raw.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "bov = BagOfWords(word2vec)\n",
    "\n",
    "idf_train = bov.build_idf(train_raw)\n",
    "train_idf = np.vstack([bov.encode(s, idf_train) for s in train_raw])\n",
    "train = np.vstack([bov.encode(s, None) for s in train_raw])\n",
    "\n",
    "idf_dev = bov.build_idf(dev_raw)\n",
    "dev_idf = np.vstack([bov.encode(s, idf_dev) for s in dev_raw])\n",
    "dev =  np.vstack([bov.encode(s, None) for s in dev_raw])\n",
    "\n",
    "idf_test = bov.build_idf(test_raw)\n",
    "test_idf = np.vstack([bov.encode(s, idf_test) for s in test_raw])\n",
    "test = np.vstack([bov.encode(s, None) for s in test_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set for the best parameter: 0.45049157303370785\n",
      "Accuracy on train set for the best parameter, with idf: 0.45482209737827717\n",
      "Accuracy on dev set for the best parameter: 0.4014532243415077\n",
      "Accuracy on dev set for the best parameter, with idf: 0.40236148955495005\n",
      "Best C no idf:6.67\n",
      "Best C with idf:4.45\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "#     In the paper, the accuracy for average of word vectors is 32.7%\n",
    "#     (VecAvg, table 1, https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "alpha = np.linspace(10**(-2), 10, 10)\n",
    "\n",
    "# no idf\n",
    "acc_train, acc_dev = [], []\n",
    "for a in alpha:\n",
    "    clf = LogisticRegression(penalty=\"l2\", C=a, solver='liblinear', multi_class='ovr')\n",
    "    clf.fit(train, Y_train)\n",
    "    acc_train.append(clf.score(train, Y_train))\n",
    "    acc_dev.append(clf.score(dev, Y_dev))\n",
    "\n",
    "# idf\n",
    "acc_train_idf, acc_dev_idf = [], []\n",
    "for a in alpha:\n",
    "    clf = LogisticRegression(penalty=\"l2\", C=a, solver='liblinear', multi_class='ovr')\n",
    "    clf.fit(train_idf, Y_train)\n",
    "    acc_train_idf.append(clf.score(train_idf, Y_train))\n",
    "    acc_dev_idf.append(clf.score(dev_idf, Y_dev))\n",
    "    \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(alpha, acc_train, '.-', label='Train set, no idf')\n",
    "plt.plot(alpha, acc_train_idf, '.-', label='Train set, with idf')\n",
    "plt.plot(alpha, acc_dev, '.-', label='Dev set, no idf')\n",
    "plt.plot(alpha, acc_dev_idf, '.-', label='Dev set, with idf')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"Regularization parameter\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "a_best = alpha[np.argmax(acc_dev)]\n",
    "a_best_idf = alpha[np.argmax(acc_dev_idf)]\n",
    "clf = LogisticRegression(penalty=\"l2\", C=a_best, solver='liblinear', multi_class='ovr')\n",
    "clf_idf = LogisticRegression(penalty=\"l2\", C=a_best_idf, solver='liblinear', multi_class='ovr')\n",
    "clf.fit(train, Y_train)\n",
    "clf_idf.fit(train_idf, Y_train)\n",
    "print('Accuracy on train set for the best parameter:', clf.score(train, Y_train))\n",
    "print('Accuracy on train set for the best parameter, with idf:', clf_idf.score(train_idf, Y_train))\n",
    "print('Accuracy on dev set for the best parameter:', clf.score(dev, Y_dev))\n",
    "print('Accuracy on dev set for the best parameter, with idf:', clf_idf.score(dev_idf, Y_dev))\n",
    "print(\"Best C no idf:\" + str(a_best))\n",
    "print(\"Best C with idf:\" + str(a_best_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "clf = LogisticRegression(penalty=\"l2\", C=a_best_idf, solver='liblinear', multi_class='ovr')\n",
    "clf.fit(train_idf, Y_train)\n",
    "Y_pred = clf.predict(test_idf).reshape(-1, 1).astype(int)\n",
    "np.savetxt(\"logreg_bov_y_test_sst.txt\", Y_pred, '%i') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on dev set:0.4123524069028156\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# Here, I used a SVM classifier on idf vectors\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# the parameters were chosen after some tuning\n",
    "clf_svm = SVC(C=1, kernel='rbf', gamma='scale')\n",
    "clf_svm.fit(train_idf, Y_train)\n",
    "print('Score on dev set:'+str(clf_svm.score(dev_idf, Y_dev)))\n",
    "\n",
    "Y_pred = clf_svm.predict(test_idf).reshape(-1, 1).astype(int)\n",
    "np.savetxt(\"svm_bov_y_test_sst.txt\", Y_pred, '%i') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Using the same dataset, transform text to integers using tf.keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# I rather use Tokenizer \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=50000)\n",
    "tokenizer.fit_on_texts(train_raw+dev_raw+test_raw)\n",
    "tokenized = tokenizer.texts_to_sequences(train_raw+dev_raw+test_raw)\n",
    "train_token = tokenized[:len(train_raw)]\n",
    "dev_token = tokenized[len(train_raw): len(train_raw)+len(dev_raw)]\n",
    "test_token = tokenized[len(train_raw)+len(dev_raw): len(train_raw)+len(dev_raw)+len(test_raw)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Pad your sequences using tf.keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "max_len = 100\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "x_train = sequence.pad_sequences(train_token, maxlen=max_len)\n",
    "x_dev = sequence.pad_sequences(dev_token, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(test_token, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Design your encoder + classifier using tensorflow.keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this container : the lookup-table, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "#     Note that the embedding layer is initialized randomly and does not take advantage of pre-trained word embeddings.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = 50000  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 32)          1600000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,625,157\n",
      "Trainable params: 1,625,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 4 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/6\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.5731 - accuracy: 0.2747\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25341, saving model to weights.hdf5\n",
      "8544/8544 [==============================] - 68s 8ms/sample - loss: 1.5731 - accuracy: 0.2748 - val_loss: 1.5681 - val_accuracy: 0.2534\n",
      "Epoch 2/6\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.4938 - accuracy: 0.3311\n",
      "Epoch 00002: val_accuracy improved from 0.25341 to 0.37965, saving model to weights.hdf5\n",
      "8544/8544 [==============================] - 53s 6ms/sample - loss: 1.4934 - accuracy: 0.3316 - val_loss: 1.4107 - val_accuracy: 0.3797\n",
      "Epoch 3/6\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.2132 - accuracy: 0.4584\n",
      "Epoch 00003: val_accuracy improved from 0.37965 to 0.39237, saving model to weights.hdf5\n",
      "8544/8544 [==============================] - 52s 6ms/sample - loss: 1.2135 - accuracy: 0.4577 - val_loss: 1.3590 - val_accuracy: 0.3924\n",
      "Epoch 4/6\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 0.9853 - accuracy: 0.5489\n",
      "Epoch 00004: val_accuracy did not improve from 0.39237\n",
      "8544/8544 [==============================] - 52s 6ms/sample - loss: 0.9850 - accuracy: 0.5485 - val_loss: 1.4530 - val_accuracy: 0.3751\n",
      "Epoch 5/6\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 0.7958 - accuracy: 0.6862\n",
      "Epoch 00005: val_accuracy did not improve from 0.39237\n",
      "8544/8544 [==============================] - 54s 6ms/sample - loss: 0.7955 - accuracy: 0.6867 - val_loss: 1.6021 - val_accuracy: 0.3842\n",
      "Epoch 6/6\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 0.6160 - accuracy: 0.7830\n",
      "Epoch 00006: val_accuracy did not improve from 0.39237\n",
      "8544/8544 [==============================] - 56s 7ms/sample - loss: 0.6162 - accuracy: 0.7831 - val_loss: 1.8247 - val_accuracy: 0.3760\n"
     ]
    }
   ],
   "source": [
    "# 5 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "#     Keras expects y_train and y_dev to be one-hot encodings of the labels, i.e. with shape=(n_samples, 5)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(Y_train)\n",
    "y_dev = tf.keras.utils.to_categorical(Y_dev)\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='weights.hdf5', verbose=1,\n",
    "                                               save_best_only=True, monitor='val_accuracy')\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, \n",
    "                    validation_data=(x_dev, y_dev), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# load best model \n",
    "model = tf.keras.models.load_model(\"weights.hdf5\")\n",
    "\n",
    "Y_pred = np.argmax(model.predict(x_test), axis = 1)\n",
    "np.savetxt(\"logreg_lstm_y_test_sst.txt\", Y_pred, '%i') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Build embedding_matrix\n",
      "Loaded 50000 pretrained word vectors\n",
      "- Fill nan values with normal distribution\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 100, 300)     15000000    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 100, 300)     0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 100, 128)     186880      spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 98, 64)       24640       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 64)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 5)            645         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 15,212,165\n",
      "Trainable params: 212,165\n",
      "Non-trainable params: 15,000,000\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.5651 - accuracy: 0.2784\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.29791, saving model to weights_.hdf5\n",
      "8544/8544 [==============================] - 188s 22ms/sample - loss: 1.5648 - accuracy: 0.2790 - val_loss: 1.5321 - val_accuracy: 0.2979\n",
      "Epoch 2/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.4549 - accuracy: 0.3515\n",
      "Epoch 00002: val_accuracy improved from 0.29791 to 0.37965, saving model to weights_.hdf5\n",
      "8544/8544 [==============================] - 170s 20ms/sample - loss: 1.4545 - accuracy: 0.3521 - val_loss: 1.3870 - val_accuracy: 0.3797\n",
      "Epoch 3/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.3707 - accuracy: 0.3875\n",
      "Epoch 00003: val_accuracy improved from 0.37965 to 0.39419, saving model to weights_.hdf5\n",
      "8544/8544 [==============================] - 171s 20ms/sample - loss: 1.3708 - accuracy: 0.3874 - val_loss: 1.3434 - val_accuracy: 0.3942\n",
      "Epoch 4/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.3346 - accuracy: 0.4115\n",
      "Epoch 00004: val_accuracy improved from 0.39419 to 0.40054, saving model to weights_.hdf5\n",
      "8544/8544 [==============================] - 171s 20ms/sample - loss: 1.3341 - accuracy: 0.4121 - val_loss: 1.3157 - val_accuracy: 0.4005\n",
      "Epoch 5/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.3050 - accuracy: 0.4280\n",
      "Epoch 00005: val_accuracy improved from 0.40054 to 0.40963, saving model to weights_.hdf5\n",
      "8544/8544 [==============================] - 171s 20ms/sample - loss: 1.3052 - accuracy: 0.4281 - val_loss: 1.2940 - val_accuracy: 0.4096\n",
      "Epoch 6/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.2666 - accuracy: 0.4481\n",
      "Epoch 00006: val_accuracy improved from 0.40963 to 0.41417, saving model to weights_.hdf5\n",
      "8544/8544 [==============================] - 171s 20ms/sample - loss: 1.2666 - accuracy: 0.4478 - val_loss: 1.3079 - val_accuracy: 0.4142\n",
      "Epoch 7/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.2469 - accuracy: 0.4545\n",
      "Epoch 00007: val_accuracy improved from 0.41417 to 0.42325, saving model to weights_.hdf5\n",
      "8544/8544 [==============================] - 171s 20ms/sample - loss: 1.2470 - accuracy: 0.4544 - val_loss: 1.2907 - val_accuracy: 0.4233\n",
      "Epoch 8/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.2197 - accuracy: 0.4712\n",
      "Epoch 00008: val_accuracy did not improve from 0.42325\n",
      "8544/8544 [==============================] - 171s 20ms/sample - loss: 1.2194 - accuracy: 0.4714 - val_loss: 1.2918 - val_accuracy: 0.4151\n",
      "Epoch 9/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.1993 - accuracy: 0.4818\n",
      "Epoch 00009: val_accuracy did not improve from 0.42325\n",
      "8544/8544 [==============================] - 170s 20ms/sample - loss: 1.1992 - accuracy: 0.4820 - val_loss: 1.3021 - val_accuracy: 0.4214\n",
      "Epoch 10/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.1762 - accuracy: 0.4979\n",
      "Epoch 00010: val_accuracy did not improve from 0.42325\n",
      "8544/8544 [==============================] - 170s 20ms/sample - loss: 1.1758 - accuracy: 0.4977 - val_loss: 1.2962 - val_accuracy: 0.4214\n",
      "Epoch 11/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.1519 - accuracy: 0.5055\n",
      "Epoch 00011: val_accuracy did not improve from 0.42325\n",
      "8544/8544 [==============================] - 171s 20ms/sample - loss: 1.1520 - accuracy: 0.5051 - val_loss: 1.3108 - val_accuracy: 0.4196\n",
      "Epoch 12/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.1172 - accuracy: 0.5282\n",
      "Epoch 00012: val_accuracy improved from 0.42325 to 0.42598, saving model to weights_.hdf5\n",
      "8544/8544 [==============================] - 168s 20ms/sample - loss: 1.1180 - accuracy: 0.5280 - val_loss: 1.3082 - val_accuracy: 0.4260\n",
      "Epoch 13/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.0917 - accuracy: 0.5417\n",
      "Epoch 00013: val_accuracy did not improve from 0.42598\n",
      "8544/8544 [==============================] - 146s 17ms/sample - loss: 1.0917 - accuracy: 0.5419 - val_loss: 1.3055 - val_accuracy: 0.4160\n",
      "Epoch 14/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.0628 - accuracy: 0.5545\n",
      "Epoch 00014: val_accuracy did not improve from 0.42598\n",
      "8544/8544 [==============================] - 128s 15ms/sample - loss: 1.0639 - accuracy: 0.5541 - val_loss: 1.3550 - val_accuracy: 0.4060\n",
      "Epoch 15/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.0323 - accuracy: 0.5754\n",
      "Epoch 00015: val_accuracy did not improve from 0.42598\n",
      "8544/8544 [==============================] - 120s 14ms/sample - loss: 1.0332 - accuracy: 0.5751 - val_loss: 1.3572 - val_accuracy: 0.4087\n",
      "Epoch 16/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 1.0103 - accuracy: 0.5764\n",
      "Epoch 00016: val_accuracy did not improve from 0.42598\n",
      "8544/8544 [==============================] - 120s 14ms/sample - loss: 1.0098 - accuracy: 0.5765 - val_loss: 1.3790 - val_accuracy: 0.4069\n",
      "Epoch 17/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 0.9772 - accuracy: 0.5963\n",
      "Epoch 00017: val_accuracy did not improve from 0.42598\n",
      "8544/8544 [==============================] - 120s 14ms/sample - loss: 0.9780 - accuracy: 0.5960 - val_loss: 1.4061 - val_accuracy: 0.4114\n",
      "Epoch 18/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 0.9490 - accuracy: 0.6103\n",
      "Epoch 00018: val_accuracy did not improve from 0.42598\n",
      "8544/8544 [==============================] - 120s 14ms/sample - loss: 0.9496 - accuracy: 0.6103 - val_loss: 1.4214 - val_accuracy: 0.4024\n",
      "Epoch 19/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 0.9010 - accuracy: 0.6335\n",
      "Epoch 00019: val_accuracy did not improve from 0.42598\n",
      "8544/8544 [==============================] - 122s 14ms/sample - loss: 0.9007 - accuracy: 0.6339 - val_loss: 1.4718 - val_accuracy: 0.4169\n",
      "Epoch 20/20\n",
      "8512/8544 [============================>.] - ETA: 0s - loss: 0.8899 - accuracy: 0.6436\n",
      "Epoch 00020: val_accuracy did not improve from 0.42598\n",
      "8544/8544 [==============================] - 120s 14ms/sample - loss: 0.8908 - accuracy: 0.6430 - val_loss: 1.4900 - val_accuracy: 0.3987\n"
     ]
    }
   ],
   "source": [
    "# 7 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Bidirectional, \\\n",
    "                                    GlobalMaxPooling1D, SpatialDropout1D, Conv1D, GRU, \\\n",
    "                                    GlobalAveragePooling1D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 20\n",
    "max_words = 50000\n",
    "embedding_size = 300\n",
    "max_len = 100\n",
    "\n",
    "# We use pretrained embeddings\n",
    "print('- Build embedding_matrix')\n",
    "\n",
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=max_words)\n",
    "embedding_matrix = np.empty((max_words, embedding_size,))\n",
    "embedding_matrix[:] = np.nan\n",
    "\n",
    "for word in word2vec.words:\n",
    "    idx = tokenizer.word_index.get(word)\n",
    "    if (idx != None) and (idx < max_words):\n",
    "        i = word2vec.word2id[word]\n",
    "        vector = word2vec.embeddings[i]\n",
    "        embedding_matrix[idx] = vector\n",
    "\n",
    "# Fill missing values with normal distribution\n",
    "print('- Fill nan values with normal distribution')\n",
    "np.random.seed(42)\n",
    "emb_not_nan = embedding_matrix[ ~np.isnan(embedding_matrix)]\n",
    "emb_mean = np.mean(emb_not_nan)\n",
    "emb_std = np.std(emb_not_nan)\n",
    "\n",
    "nb_nan = embedding_matrix[ np.isnan(embedding_matrix)].shape[0]\n",
    "embedding_matrix[ np.isnan(embedding_matrix)] = np.random.normal(emb_mean, emb_std, nb_nan)\n",
    "\n",
    "# and then, build a model based on a BiLSTM layer concatenate with a Conv1D layer\n",
    "\n",
    "input_ = Input(shape=(max_len, ))\n",
    "x = Embedding(max_words, embedding_size, weights=[embedding_matrix],trainable = False)(input_)\n",
    "x = SpatialDropout1D(0.1)(x)\n",
    "x = Bidirectional(LSTM(64, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool]) \n",
    "preds = Dense(5, activation=\"sigmoid\")(x)\n",
    "model = Model(input_, preds)\n",
    "model.summary()\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='weights_.hdf5', verbose=1,\n",
    "                                               save_best_only=True, monitor='val_accuracy')\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, \n",
    "                    validation_data=(x_dev, y_dev), callbacks=[checkpointer])\n",
    "\n",
    "model = tf.keras.models.load_model(\"weights_.hdf5\")\n",
    "\n",
    "Y_pred = np.argmax(model.predict(x_test), axis = 1)\n",
    "np.savetxt(\"conv1D_lstm_y_test_sst.txt\", Y_pred, '%i') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
